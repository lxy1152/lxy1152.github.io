---
title: 🧛‍♂️ I/O
---

## I/O 多路复用<Badge text="重点" type="error"/>

### Socket 模型的演进？

想要在不同的主机之间发送信息需要使用 Socket，一个最基本的模型如下图所示：

:::center
![socket模型.png](https://i.loli.net/2021/08/01/YnOwgEstVAXGMrN.png)
:::

但是这个模型只提供了一对一的服务，因为它使用的是阻塞 I/O，当服务端在还没处理完一个客户端的网络 I/O 时，或者读写操作发生阻塞时，其他客户端是无法与服务端连接的。为了使服务端能够对多个客户端提供服务，需要对上述模型进行改造，一种最简单的思路是：每有一个请求就创建一个线程为它提供服务。这种用多个进程来应付多个客户端的方式，在客户端数量比较少的时候还是可行的，但是当客户端数量很大，比如一万、十万时，肯定是扛不住的，因为每产生一个进程，必会占据一定的系统资源，除此之外还需要考虑进程间上下文切换的性能损耗。

:::center
![socket模型1.png](https://i.loli.net/2021/08/01/yhkdLPsqU8tb6nM.png)
:::

为了解决这个问题，可以引入线程池，通过使用线程池来避免线程的频繁创建和销毁。所谓的线程池，就是提前创建若干个线程，当有新连接建立时，将这个已连接的 Socket 放入到一个队列里，然后线程池里的线程负责从队列中取出已连接的 Socket 进程进行处理。

:::center
![socket模型2.png](https://i.loli.net/2021/08/01/CHlZTdQpyxcSJrz.png)
:::

上述的方案都可以解决，但都不是最好的方案。真正的解决方案是 I/O 多路复用。

### 什么是 I/O 多路复用？

I/O 多路复用是指使用一个进程来维护多个 Socket，一个进程虽然在任一时刻只能处理一个请求，但是处理每个请求的事件时，耗时控制在 1 毫秒以内，这样 1 秒内就可以处理上千个请求，把时间拉长来看，多个请求复用了一个进程，也就是多路复用，这种思想很类似一个 CPU 并发多个进程。

I/O 多路复用主要涉及三个系统调用：select、poll、epoll。前两个基本相同，主要使用的是最后一个。

### 什么是 select 和 poll？

select 实现多路复用的方式是将已连接的 Socket 都放到一个文件描述符集合中，然后调用 select 函数将文件描述符集合拷贝到内核里，让内核来检查是否有网络事件产生。检查的方式很粗暴，就是遍历，当检查到有事件产生后，将此 Socket 标记为可读或可写， 接着再把整个文件描述符集合拷贝回用户态里，然后用户态还需要再通过遍历的方式找到可读或可写的 Socket，再对其进行处理。

select 使用固定长度的 BitsMap，表示文件描述符集合，而且所支持的文件描述符的个数是有限制的，在 Linux 系统中，由内核中的 FD_SETSIZE 进行限制， 默认最大值为 1024，只能监听 0~1023 个文件描述符。

poll 不再用 BitsMap 来存储所关注的文件描述符，取而代之用动态数组，以链表形式来组织，突破了 select 的文件描述符个数限制，当然还会受到系统文件描述符限制。

但是 poll 和 select 并没有太大的本质区别，都是使用线性结构来存储进程关注的 Socket 集合，因此都需要遍历文件描述符集合来找到可读或可写的 Socket，时间复杂度为 $O(n)$，另外还需要在用户态与内核态之间拷贝文件描述符集合，这种方式随着并发量的增长，性能的损耗会呈指数级增长。

### 什么是 epoll？

epoll 在两个方面进行了改造，很好解决了 select/poll 的问题。

#### 怎么保存所有待检测的文件描述字？

epoll 在内核里使用红黑树来跟踪进程所有待检测的文件描述字，把需要监控的 socket 通过 `epoll_ctl()` 函数加入内核中的红黑树里。红黑树是个高效的数据结构，增删查的时间复杂度是 $$O(\log N)$$，避免了像 select/poll 操作一样每次都传入整个 Socket 集合，减少了内核和用户空间大量的数据拷贝和内存分配。

#### 怎么知道存在 Socket 需要处理？

epoll 使用事件驱动的机制，在内核里维护了一个链表来记录就绪事件，当某个 Socket 有事件发生时，通过回调函数内核会将其加入到这个就绪事件列表中，当用户调用 epoll_wait() 函数时，只会返回有事件发生的文件描述符的个数，不需要像 select/poll 那样轮询扫描整个 Socket 集合，大大提高了检测的效率。

:::center
![epoll.png](https://i.loli.net/2021/08/01/JcjvrEgOleKW1Ps.png)
:::

### epoll 的两种触发模式？

- 边缘触发模式：当被监控的 Socket 描述符上有可读事件发生时，服务器端只会从 epoll_wait 中苏醒一次，因此程序要保证一次性将内核缓冲区的数据读取完，通常和非阻塞 I/O 搭配使用
- 水平触发模式：当被监控的 Socket 上有可读事件发生时，服务器端不断地从 epoll_wait 中苏醒，直到内核缓冲区数据被 read 函数读完才结束

- 对于 select 和 poll，它们只提供了水平触发模式，epoll 默认也是水平触发方式，但也可以配置为边缘触发模式。

## Reactor 模型

### 什么是 Reactor？

Reactor 指的是对事件的反应，也就是说来了一个事件，Reactor 就有相对应的响应。事实上，Reactor 模式也叫 Dispatcher 模式，即收到事件后，根据事件类型分配（Dispatch）给某个进程 / 线程。

Reactor 模式主要有以下四种实现方案：

- 单 Reactor 单进程 / 线程
- 单 Reactor 多进程 / 线程
- 多 Reactor 单进程 / 线程
- 多 Reactor 多进程 / 线程

其中，多 Reactor 单进程 / 线程的实现方案相比单 Reactor 单进程 / 线程的方案，不仅复杂而且也没有性能优势，因此实际中并没有应用。主要介绍一下另外三种方案。

### 介绍一下单 Reactor 单进程/线程模型？

:::center
![reactor1.png](https://i.loli.net/2021/08/01/3ZNEiHUuoLj7gnK.png)
:::

这种方案存在两个缺点：

- 因为只有一个进程，所以无法充分利用 多核 CPU 的性能
- Handler 对象在业务处理时，整个进程是无法处理其他连接的事件的，如果业务处理耗时比较长，那么就会造成响应的延迟
  
因此，这种方案更适合于业务处理非常快速的场景。比如 Redis（Redis 的瓶颈不在 CPU，而是内存和网络 I/O）。

### 介绍一下单 Reactor 多进程/线程模型？

:::center
![reactor2.png](https://i.loli.net/2021/08/01/A1cW5D4xuwqJoft.png)
:::

单 Reator 多线程的方案优势在于能够充分利用多核 CPU 的性能，既然引入多线程，那么自然就带来了多线程竞争资源的问题。同时因为只有一个 Reactor 对象承担所有事件的监听和响应，在面对瞬间高并发的场景时，容易成为性能瓶颈。

### 介绍一下多 Reactor 多进程/线程模型？

:::center
![reactor3.png](https://i.loli.net/2021/08/01/7rWqT5QtOoCKNSZ.png)
:::

多 Reactor 多线程的方案虽然看起来复杂的，但是实际实现时比单 Reactor 多线程的方案要简单的多，原因如下：

- 主线程和子线程分工明确，主线程只负责接收新连接，子线程负责完成后续的业务处理
- 主线程和子线程的交互很简单，主线程只需要把新连接传给子线程，子线程无需返回数据，直接就可以在子线程将处理结果发送给客户端
  
使用这种方案的包括 Netty，Memcached，Nginx。

## Proactor

### 什么是同步/异步，阻塞/非阻塞？

阻塞是指当用户程序执行 read 的时候，线程会被阻塞，一直等到内核数据准备好，并把数据从内核缓冲区拷贝到应用程序的缓冲区中，即拷贝过程完成时，read 才会返回。所以阻塞等待的是内核数据准备好和数据从内核态拷贝到用户态这两个过程。

:::center
![阻塞io.png](https://i.loli.net/2021/08/01/kTFSnVzM8Av3wY9.png)
:::

非阻塞 I/O 的 read 请求在数据未准备好的情况下会立即返回，并会继续往下执行，此时应用程序不断轮询内核，直到数据准备好，read 调用才可以获取到结果。但是最后一次 read 调用，获取数据的过程，是一个同步的过程。这里的同步指的是内核态的数据拷贝到用户程序的缓存区这个过程。过程如下图：

:::center
![非阻塞io.png](https://i.loli.net/2021/08/01/9jSit8LTUMm57PA.png)
:::

异步 I/O 指的是内核数据准备好和数据从内核态拷贝到用户态这两个过程都不用等待。当一切处理完成后，由操作系统通知应用程序过来处理。

:::center
![异步io.png](https://i.loli.net/2021/08/01/y3VUMcO29xsPS5C.png)
:::

### 什么是 Proactor？

Reactor 是非阻塞同步网络模式，而 Proactor 是异步网络模式。流程图如下：

:::center
![proactor.png](https://i.loli.net/2021/08/01/8RxVgNHO5qJnXtp.png)
:::

可惜的是，在 Linux 下的异步 I/O 是不完善的，aio 系列函数是由 POSIX 定义的异步操作接口，不是真正的操作系统级别支持的，而是在用户空间模拟出来的异步，并且仅仅支持基于本地文件的 aio 异步操作，网络编程中的 Socket 是不支持的，这也使得基于 Linux 的高性能网络程序都是使用 Reactor 方案。

而 Windows 里实现了一套完整的支持 Socket 的异步编程接口，这套接口就是 IOCP，是由操作系统级别实现的异步 I/O，真正意义上实现了异步 I/O，因此在 Windows 里实现高性能网络程序可以使用效率更高的 Proactor 方案。

### 和 Reactor 的对比？

Reactor 可以理解为：来了事件操作系统通知应用进程，让应用进程来处理；而 Proactor 可以理解为：来了事件操作系统来处理，处理完再通知应用进程。这里的事件可以指：新连接、有数据可读、有数据可写的这些 I/O 事件。这里的处理指从驱动读取到内核以及从内核读取到用户空间。
